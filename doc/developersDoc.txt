"""
MetPX Copyright (C) 2004-2006  Environment Canada
MetPX comes with ABSOLUTELY NO WARRANTY; For details type see the file 
named COPYING in the root of the source directory tree.
"""
=================================================================================
=      _                _                          ______           
=     | |              | |                         |  _  \          
=   __| | _____   _____| | ___  _ __   ___ _ __ ___| | | |___   ___ 
=  / _` |/ _ \ \ / / _ \ |/ _ \| '_ \ / _ \ '__/ __| | | / _ \ / __|
= | (_| |  __/\ V /  __/ | (_) | |_) |  __/ |  \__ \ |/ / (_) | (__ 
=  \__,_|\___| \_/ \___|_|\___/| .__/ \___|_|  |___/___/ \___/ \___|
=                              | |                                  
=                              |_| 
=                             
=
= Author          : Nicholas Lemay
= Last Updated    : August 14th 2007
=
=
==================================================================================


About this document :
==================================================================================

This file was written for anyone using the stats library who might be interested 
in it's inner workings. Developers and end-users alike will be able to 
understand the principles behind the development of this package. 

Special care was given to describing the requirements and observations that were 
made during development as to guide someone who might be interested in modifying 
this library. 

Hopefully this will explain why things were done a certain way and keep
the developper from doing the same errors that were made during initial 
development.   


pxStats package primer :
==================================================================================
The stats package's main goal is to manage stats regarding MetxPx's different 
clients/sources and to add the possibility to to draw graphics based on said 
stats.

The graphic's data is based on the data calculated from log files produced by
MetPx wich can be remotely located.


DEVELOPMENT REQUIREMENTS :
==================================================================================

- The package has to be able to gather log files from remote locations( machines ). 

- The package has to be able to read a specific format of log files.

- The package must allow user to collect data from log files wich are constantly 
  growing. 

- The package must permit data collection of log files that contains information
  about the same source/client but that comes from numerous machine sources. 
  Each can be remotely located. 
    
- The package must have a way to save the collected data.
  Data needs to be stored in an efficient manner so that 
  we can keep data for up to 10 years. 

- Minimally the package must be able to collect the following data types: errors, 
  bytecount and latency.

- With these data types, the packages must be able to compute the totals or mean 
  of each types. It must also keep count count of the number of files handled.
  Of that number of files it also must keep count of the number of files for
  wich the latency exceeds a certain maximum.  
  
- The package must have a way to save wich product type is associated with 
  each line collected.  

- Data collection and saving must be done as quickly as possible. This must also
  be done without affecting the machine on wich it is being run too much. 

- The package must be able to produce graphics of the .png format based on the 
  saved data.

- The package must be able to display such graphic to a user within an
  acceptable time frame.( < 40 sec )    
  
- The package must give access to graphics through the Columbo web interface. 
 
- The package must allow user to specify the following options for data
  collection and graphics production : machine name, current time, 
  source/client name, file type, product type.
  
- The package must allow the user to creates client or source groups wich are
  based on existing clients and sources. The package must be able to treat 
  the groups just like individuals client/sources and allow all of the 
  available functionalities to be used with the groups. 

- Library must be built in such a way that all the above action can be done
  on an automated basis i.e. be called by crontabs.
  
- Library must have mechanisms that allow data collection to be reverted in 
  case errors are found.
  
- Library must have a monitoring tool that limits the number of verifications
  made by the administrators to a bare minimum.   
 
- Library must give access to the archives of produced graphics through a 
  web page.  

- Library must allow users to generate graphics on the fly via a web
  form.  
  
  
  
GUIDING PRINCIPLES
==================================================================================
- Log files for a specific source/client are often stored on many different
  machines.  

- Log file for a single day from a single machine can be quite large. 
  ( average~ >30 megs )

- Log file names aren't reliable. Even if a file has a certain date in it's
  name it doesn't necessarily mean it will contain data for that day. File 
  content still need to be investigated to make sure it does or doesn't contain
  usefull info.   

- Having every machine perform the collection of the data found in the files
  that are present in it's own space makes the application more scalable
  in the event that more machines and or sources/clients are to be added. 
  It also speeds up the collection since log filesdo not require to be 
  transferred. 
  
- On the other hand, collecting the data on the machines producing the log files 
  instead of downloading the log files into another machine can impact the machine
  to a much greater degree.  
  
- Collecting a whole days worth of data and producing stats according to data
  collected is very long. Therefore, more frequent data collection should be made.
  Hourly collection seems optimal for logical and performance reasons.  
  
- Disk access is very slow. Anything that has to be loaded or saved on the disk
  slows down the entire process quite a lot. 
  
- For instance, even with some optimizations, on a users machine the time 
  taken to pickle a single hour of data using a single pickle file every day
  went from three seconds ( maximum ) for the first few hours to almost 
  10 seconds( maximum ) for the last hours of the day.   
    
- Because of the preceding, we've found out that using a single daily file 
  would make it impossible to meet the above requirements.     
  
- Pickling done during the final hour of the day on a machine containing 150
  sources/clients would mean roughly 150 * 10 sec = 1500 sec ~ 25 minutes
  This was way too much time dedicated to collecting data.
  
- Saving pickled data into hourly files makes sure that time used for pickling
  remains stable throughout the day. It also makes the total amount of time 
  much more acceptable. 
  
  On a normal machine it would mean ~3 sec * 150 = 450 450/60 = 7.5 minutes
  ( worst case scenario )
  
- On more performant dev machines where the app is being tested, it takes a 
  maximum of less than 2 second to update a single source/client.
 
  Currently, with tests on 2 developments machines, wich have  70 active
  sources/clients it takes less than 2 minutes to create all the hourly pickles
  for the 70 sources/clients.          

- These test machine all have numerous processors. Since data collection times 
  have been brought down to very quick times it has been decided not to launch 
  any more process to try and speed up the collecting since it might slow down
  other applications for a minimal performance boost. 

- Gnuplot graphics are generated rather quickly but still take a few seconds
  each( < 5seconds ). 

- Therefore it has been decided that multi-processing would be usefull when 
  producing graphics. Number of simultaneously ran process' can be modified as
  to not increase machine load too much.

- For the same reasons multi-processing has been implemented on transfers from 
  pickle files to rrd databases.



IMPLEMENTATION :
==================================================================================

Remote file handling ( rsync )
------------------------------------------------------------------

On our current configuration data collection was not made directly on the machines 
containing the logfiles. Therefore log files were downloaded to another machine 
using rsync. 

rsync has been chosen since it will only download part of the files
that are missing when files are allready found on destination machine.
 
It also offers a --delete option wich deletes files on the destination machine
wich are no longer present on the source machine. This allows for a picture 
perfect mirror of the source, while saving us the trouble of implementing an 
outdated-file purging mechanisn.   

The following line is usually used to get and exact copy of the log folder of a
certain machine into the desired machines.

rsync -avzr --delete-before  -e ssh login@machine/PATH_TO_LOG_FILES PATH_TO_LOGFILES  

The --delete-before option will delete from the local folder any file that is 
no longer present in the folder we want to mirror. SSH option will allow for 
an ssh connection to be made between machines.

The avzr are standard rsync option meaning that download should be recursive(r),
verbose(v), compressed(z) and archive(a).

File synchronisation is used between pickle producing machines and graphic producing
machines( pickleSynchroniser.py ) and between the machine running the sources/clients 
(wich produces log files) and the machine doing the pickling.
 

Data collection :
---------------------  

- To speed up data collection and stats production, data collected is saved 
  in a pickle files. This way, data for a specific timeframe only has to be
  collected and calculated once. This can save enormous amounts of time on 
  graphic production if the timespan of the graphic asked by the user is of any
  importance. 
  
  FilesStatsCollector.py, DirecToryStatsCollector.py, pickleUpdater.py and 
  cpickleWrapper.py all contain methods to deal with data pickling and pickled data.
     
- Because of disk access times, data pickling can be rather slow. To speed it up
  cpickle has been added to the library. cpickle is a c based implementation of 
  pickle that has much faster methods than the usual pickle.
  
- Data collection can be done on an automated basis using crontab to call 
  pickleUpdater. Note : Main script named pxStatsStartup does call pickleUpdater.

- Data colection can be made on growing files. This is true as long as the top 
  of the file remains intact and that data is only appended at the end of the 
  log files. If top of the file is modified, the saved positioning of the file 
  will be corrupted and reading the file will probably produce errors.  

- File positioning of the last read file is saved for every sources/clients in 
  the /apps/px/pxStats/logfileAccess pickle file.  
  
- By default all data types are collected. This is done so that there will be no 
  problem if a user wants to produce any kind of graphic for a certain 
  source/client.

- User can specify wich data types he wants collected but that will limit user
  choices for graphics.  
  

Gathering pickled files:
------------------------

- Remotely located pickle files are to be transfered to the machine where the
  graphics are to be created. see pickleSynchroniser.py for details.           


Archiving the pickled data.
-------------------------------

The data contained in pickled files takes up a lot of space and contains a lot 
of data that is usefull for short amount of time only, and that can be discarded
when viewing data for wide amounts of time. We have therefore decided to archive
our pickled data by transferring it into rrd databases.

Such databases have a fixed size so they are very space efficient when using 
data that spans over a large amount of time and that precise information is not
needed. rrdTool also offers to efficiently plot graphics(rrdgraph) based on the 
data contained in an rrd.   

To transfer data into rrd databases( transferPickleToRRD.py ) and plot graphics
with rrdgraph( generateRRDGraphics.py) you will need to have the rrdtool python 
library installed on your machine.

See rrdToolDoc.txt for more details on rrdTool.   


Graphic production
-------------------

- Graphics with a short timespan are to be produced using StatsPlotter.py. 
  It's simply a modified version of Plotter.py wich was allready working and
  available in the library. This file uses gnuplot to generate the graphic
  so the gnuplot library is required here.    
     
- Graphics with longer timespans can be drawn using generateRRDGraphics.py
  This files uses rrdtool rrdgraph program to draw its graphics. These graphics 
  contain less detail and can only display one graphic per image but since our 
  rrd database will contain data for up to the last ten years it will allow
  users to easily produce daily, weekly, monthly and yearly graphics.
    
- It is possible to produce graphics for each data type collected. This means
  latency, bytecount and errors.  

- It is impossible to create a graphic on a data type wich was not previously
  collected. This is why by default all types are collected and pickled. This 
  allows users to have access to every data type possible. 



Dealing with remote machines
-----------------------------

- To make the application more scalable in terms of a possible 
  growing number of machines it can be decided that every machines 
  would do it's own data pickling using pickleUpdater. Each machine 
  will require to have the entire pxStats package installed. 

- Some sources/clients have their data stored on different machines. 
  pickleMerging.py has been introduced in the library to make merging of 
  different machines pickles possible.  

- Since the graphic producing machine is at the heart of the library, it is 
  this machine that will launch all the updates on the remote machines 
  containing the log files. 
  
  It will also be the file containing all the configuration files that
  will needto be filled in order to make all the machiens work properly.
   
  
- remotely executed commands using ssh need to be run from the graphic producing machine.
    

- The graphic producing machine also needed access to all the pickles created
  remotely as to generate graphics. This prompted the implementation of a rsync
  system (pickleSynchroniser.py) that allows pickled data found on numerous 
  machines to be brought back in a single folder of a specific machine if needed.

- The graphic machine has no way of knowing if it is using the right version of 
  the pickle files. Therefore we have implemented a file version checking program
  that uses the time of creation of the pickle files to see if a newer version has
  appeared on the pickleCreation machine. If so the graphic generation program
  will use the new file next time a graphic using this pickle file is asked 
  for by a user.  


Graphic interface :
------------------------------------------------------------------

Columbo  
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Up to date, 24 hours wide graphics are made available 
    to users using a simple link represented by a small
    graphic image within Columbo's PX Circuits tab. 

    
--> Daily, weekly, monthly and yearly web pages.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
    The other graphics are avaible on the following web 
    pages : dailyGraphs.html, weeklyGraphs.html, 
    monthlyGraphs.html and yearlyGraphs.html.
    
    Since these web page only need to be updated 
    upon graphics updates, we (re)generate these 
    web pages every time the pxStatsStartup.py  
    script is being run.

    -Daily graphs gives users the acces to the daily graphics of the past 7 days. 
    -Weekly graphs gives users the access to the weekly graphs of the past 5 weeks.
    -Monthly graphs gives users the access to the monthly graphs of the past 3 months.
    -Yearly graphs gives users the access to the yearly graphs of the past 3 years.

--> Combined data web pages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    A special section is given to every machine group that 
    are found within the configuration file. For each of 
    these groups, a web page is generated wich displays 
    graphics based on the combined data of all the sources
    or clients wich are currently running on that machine group.
    
    These pages are also statically produced every time 
    the pxStatsStartup.py script is being run. 


--> Request interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    A special cgi webPage named graphicsRequestPage.py 
    was designed to allow users to generate graphics 
    on the fly.
    
    This interface contains a form that displays all 
    the options available for producing graphics based
    on the selected plotter.
    
    Graphics are then generated using Ajax requests 
    and shown at the bottom of the very same web page.


Monitoring the library's activities :
--------------------------------------------------------------

Most parts of the library were meant to be used with automated
calls. This means users would probably prefer to let things run
and not have to worry about it. Furthermore, the library stores 
it's data within a 10 years wide database. If erronous data 
infiltrates the database, the entire set of data will be slightly
corrupted. 

This is why the statsMonitor.py program was created. It was 
designed to be ran on a regular interval to keep track of activities 
within the stats library and to warn the software's administrator 
to make the needed modifications.  



List of files and folders included in the package are :
--------------------------------------------------------------
.. image:: arbo.png
.. image:: python.png
.. image:: lib.png
.. image:: web.png


.../bin
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    __init__.py     
    generateGraphics.py                     
    generateRRDGraphics.py     
    generateAllGraphsForServer.py   
    pickleUpdater.py             
    pxStatsStartup.py  
    statsMonitor.py  
    transferPickleToRRD.py       
   
.../bin/debugTools
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    _init__.py  
    pickleViewer.py
    picklesTimeOfUpdatesViewer.py
    setTimeOfLastUpdates.py


.../bin/webPages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    dailyGraphicsWebPage.py  
    getGraphicsForWebPages.py 
    generateTopWebPage.py    
    graphicsRequestBroker.py   
    graphicsRequestPage.py 
    monthlyGraphicsWebPage.py 
    totalGraphicsWebPages.py   
    weeklyGraphicsWebPage.py
    yearlyGraphicsWebPage.py
    
.../bin/tools
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    __init__.py
    archiveGraphicFiles.py  
    backupRRDDatabases.py
    clean_dir.plx                     
    fileRenamer.py     
    fileVersionsAndAccessCleaners.py
    pickleCleaner.py  
    pickleSynchroniser.py
    restoreRoundRobinDatabases.py
    retreiveDataFromMachine.py
    
        
.../data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~        
        databases        
        fileAccessVersions
        graphics 
        logFileAccess 
        logFiles 
        monitoring  
        pickles
        picklesTimeOfUpdates 
        previousMachineParameters
        webPages     
        
         
.../doc
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    developersDoc.txt
    howTo.txt
    ifSomethingGoesWrong.txt
    installation.doc
    monitoringDoc.txt
    rrdToolDoc.txt
    
    
           
.../etc
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    clean_dir.conf
    config 
    configForMachines
    monitoringConf
    pxConfigFiles
    
    
.../lib
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    __init__.py
    BackwardReader.py                                                                                         ClientGraphicProducer.py         
    ClientStatsPickler.py                     
    CpickleWrapper.py                       
    DetailedStatsParameters.py                       
    FileStatsCollector.py                                 
    GeneralStatsLibraryMethods.py
    GnuQueryBroker.py
    GraphicsQueryBrokerInterface.py
    GroupConfigParameters.py  
    LogFileAccessManager.py
    LogFileCollector.py 
    MachineConfigParameters.py
    MemoryManagement.py
    PickleMerging.py
    PickleVersionChecker.py
    RRDQueryBroker.py
    RrdUtilities.py
    StatsConfigParameters.py
    StatsDateLib.py 
    StatsMonitoringConfigParameters.py
    StatsPaths.py
    StatsPlotter.py
    TimeParameters.py

.../man
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.../man/man1/
:::::::::::::::::::::::::::::::::::::::::::::::::::::::
    backupRRDDatabases.1
    dailyGraphicsWebPage.1   
    generateGraphics.1                          
    generateRRDGraphics.1
    getGraphicsForWebPages.1
    launchGraphCreation.1                   
    monthlyGraphicsWebPage.1          
    pickleCleaner.1 
    pickledTimesViewer.1
    pickleSynchroniser.1 
    pickleUpdater.1    
    pickleViewer.1   
    restoreRoundRobinDatabases.1
    statsMonitoringConfiguration.1
    setTimeOfLastUpdate.1     
    statsConfig.1  
    statsConfigFormachines.1
    statsMonitor.1               
    transferPickleToRRD.1
    weeklyGraphicsWebPage.1
    yearlyGraphicsWebPage.1


Note : Exception notwithstanding, files that start with a lower case
       character are executable programs.
       Files that start wiht an upper case character contain contain class'
       to be used as objects.

       
       
These library files are required by the stats package :
-------------------------------------------------------------------
    
    RRDTOOL    : Time-series storage and display system.
    Python-rrd : Python package.
    Gnuplot.py : Python package.

    
    
Environment variables :
---------------------------------------
Environment variables no longer need to 
be set in the current version of the 
stats package. 



File interaction (): 
--------------------------------------------------------------------------------

Main program :
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                                   
     
--Note : x-->y means x uses y            




Structure of the graphics requests : 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



                                    


File system 
--------------------------------------------------------------------------------
 

Pickle Files :
~~~~~~~~~~~~~~~~~~~~~~~~~~

pxStats/data/picklesTimeOfUpdates/fileTypeClientNameMachineName
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    This is a folder created to contain all the last pickle updates times 
    from each of the different sources/clients. The files within
    the folder are saved in a regular pickle file format.
    
    This is done so that an administrator could manually modify 
    a certain client's date if needed.


pxStats/data/fileAccessVersions/clientName_machineName:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Contains a list of file update time saved by PickleVersionChecker.
See PickleVersionChecker.py for details.


pxStats/data/pickles/clientOrSourceName/YYYYMMDD/fileType/machineName_hh
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

These file are created to save the data collected for the period covering
the hour specified in the file's name.  This file contains a FileStatsCollector
instance containing all the data collected for that hour saved with
cpickleWrapper.py.
 
These files will be grouped in a folder named pickles. This folder will contain
many subfolder, all of wich named after a certain source/client name. These 
folders will contain all the pickle files of that source/client.  
             


Gnuplot Graphics :
~~~~~~~~~~~~~~~~~~~~~~~~~~

pxStats/graphics/others/gnuplot/clientName/
fileType_clientName_YYYYMMDD_HH:MM:SS_dataTypes_XXhours_on_server_products.png
~~~~~~~~~~~~~~~~~~~~~~~~~~

The files are created everytime a graphic is asked for by a user. This will be 
the png image containing all the graphics asked for by the user during his 
resquest.  It will contain graphs for each of the data types asked for as many
sources/clients as specified. 

These files will be grouped in a folder named graphs. This folder will contain 
many subfolder, all of wich named after a certain source/client name. These 
folders will contain all the image files of that client.


   

   
RRD graphics :
--------------------------------------------------------------------------------
RRD graphics are to be used to plot graphs whose starting time is older than the 
time for wich we keep pickle files backed up.

RRD databases as they are presently set keep data for the past 10 years. This
means that at any time user can request data for any date in the past 10 years.
See RRD section for more details.

RRD graphics are stored in this fashion : 
pxStats/data/graphs/others/rrd/clientName/
fileType_clientName_YYYYMMDD_HH:MM:SS_dataTypes_XXhours_on_server.png



Graphics for the different web pages.
--------------------------------------------------------------------------------

Currently, web pages are created as to display the daily, weekly, monthly and 
yearly graphics of the different client/sources.

Graphics used by theses web pages are stored in this fashion :
pxStats/data/graphics/webGraphics/timeSpan/type/clientOrSourceName/fileName 

Where timeSpan is either daily, weekly, monthly or yearly and where type is either bytecount, errors, filecount, filesOverMaxLatency or latency.

File name style will be Mon.png, Tue.png, Wed.png etc for daily graphs.   
File name style will be 01.png, 02.png, 03.png etc for weekly graphs.
File name style will be Jan.png, Feb.png, Mar.png etc for monthly graphs.
File name style will be 2005.png, 2006.png, 2007.png etc for monthly graphs.


Databases
----------

pxStats/data/databases/currentDatabases/type/client_machinename
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
These files are created when user asks for data to be transferred from pickle
files to an rrd database. Type will be either bytecount, errors, filecount,
filesOverMaxLatency or latency.
      



/apps/px/pxStats/databases/currentDatabasesTimeOfUpdates/fileType/client_machineName
~~~~~~~~~~~~~~~~~~~~~~~~~~
These files are pickle files containing the time of the last database update
of the databases of a certain client/sources. 


Important notes ( Specifics )
------------------------------

When using the pickleUpdater to make automatic updates, data collection will 
start where the last data Collection was made. If no pickling at all was made 
for that source/client, it will start at xx:00:00 of that the hour specified.   

Data pickling a source/client thats not listed in picklesTimeOfUpdates can only be 
done within the same hour. If data needs to be collected in a previous hour, you
need to specify the day in the call to pickleUpdater call. 
***See usage for details.  

Data pickling can otherwise be done over numerous days. This means that if no
pickling occured for a few days for some reason, pickling can be resumed like 
nothing happened although first pickle update will be quite long. 

While using the higher level pickleUpdater and ClientGraphicProducer classes,
time buckets and pickle files will be created on a much more rigid daily basis.

    - A FileStatsCollector instance will be created every hour and contain time
      buckets starting at xx:00:00 that hour and ending at xx:59:59. 
    
    - Pickle will dave each daily instance in a file named after the source/client
      name and the date of the pickle. 
      
    - Every hour, a new pickle will be created to contain the new 
      FileStatscollector entry.     


Graphics cannot be produced for a data type wich was not previously collected
for that source/client. 

If need be, picklesTimeOfUpdates entries can be modified so that library thinks
the last pickle update occured at a different time then was written.           
        

USAGE:
--------------------------------------------------------------------------------

1- See the howTo.txt file contained in this folder.
2- Most files whos name starts with a lower case letter can be executed with a 
   -h option to get usage.
3- All the code within .py files is rather heavily commented.


Todo, bugs etc....
--------------------------------------------------------------------------------
None for the moment
       





 

 
 
