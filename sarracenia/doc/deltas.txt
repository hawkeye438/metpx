
for files that change:

algorithm assumed:
	- for each ´block´ (blocksize interesting) generate a signature.
	- when a subscriber reads an announcement, it includes the signature.
	- he compares the signatures on the file he already has, and updates it to match.


the zsync algorithm is the right idea, can perhaps use it directly.

----------- what if each notification is for a block, not a file ?--------------

what if the messages we send are all per block?
say we set a blocksize of 1MB, and we checksum that block, noting the offset, then
continue?

so v01.notify...  

gedanken experiment... per block messages, rather than entire files ?

has 
v01.notify:
blocksz sz-inblocks remainder blocknum chksum base-url relative-path ....

The blocksz establishes the multiplier for the sz and the blocknum.  the remainder
is the last bit of the file after the last block boundary.

So you calculating the checksum for each block you send off a message with the block, 

this way, for large files, the transfer can be split over a large number of nodes.
but then re-assembly is a bit of a puzzle.  will each node of ddsr have only
fractional (aka sparse) large files?   as long as the dd_sub is to both ddsr's, it should
get everything.   what happens with sparse  files?

https://administratosphere.wordpress.com/2008/05/23/sparse-files-what-why-and-how/

it's OK...
it would work, but it's a bit strange, and how do we know when we are done?
How about the following.  when dd_subscribe writes files, it writes to a file
suffixed .part when it receives a file, and there is a .part it checks the size
of the file, if the current part is contiguous, it just appends the data to the .part
file.  If not, it creates a separate .part<blocknum>

Then it looks in the directory to see if a .part<blocknum +1> exists, and appends
it if it does, and loops until all contiguous parts are appended (the corresponding
files deleted.)  

NOTE: Do not use append writing .part, but always lseek and write.  This prevents 
race conditions from causing havoc.  If there are multiple dd_subscribes writing 
the file they will just both write the same data multiple times (worst case.)

anyways when you run out of contiguous parts, you stop.

if the last contiguous block received includes the end of the file, then
do the file completion logic.

Why is this really cool?  It does the gridftp thing, splitting out single
file transfers into parallel streams.

so this means that for large files, the ddsr's might have a whole bunch of
part files, instead of the complete ones, because the transfer is split over
multiple nodes,  no problem, as long as later stages are subscribed to all
ddsr's.

This also deals with files that are written over time, without waiting
until they are complete before hitting send.

how to pick chunksize...
	- source choice?
	- we impose our own on ddsr?
	
a default to 10*1024*1024=10485760 bytes, with override possible.

-------------------------------------------------------------------------


----------- digression about zsync ------------------
zsync is available in repositories.  
and zsync(1) is the existing download client.  
zsyncmake(1) builds the signatures, with a programmable block size. 

It looks ike zsync is usable as is?

downside:  portability.
    need zsync on windows and mac for downloads, dependency a pain.
	there is a windows binary, made once in 2011... hmm...
	have not seein it on Mac OS either... sigh...

we send the signatures in the announcements, rather than posting on the site.
If we set the blocksize high, then for files < 1 block, there is no signature.

should dd_sara post the signature to the site, for zsync compatibility?
	-- write a .zsync file?

I hate forking...  don´t want to be forking zsyncmake for every product...
even if we do not use zsync itself, might want to be compatible... so use
a third party format and have a comparable.  1st implementation would do
forking, 2nd version might replicate the algorithm internally.

perhaps we have a threshold, if the file is less than a megabyte, we just send
the new one. if it is bigger, 
zs

The intent is not to replicate source trees, but large data sets.  
	- for most cases (when writing a new file) we do not want extra overhead.
	- target is large files that change, for small ones, transfer again, is not a big deal.
	- want to minimize signature size (as will travel with notifications.)
	- so set a block size to really large.


perhaps build the zsync client into dd_subscribe, but use zsync make on the server side ?
or when the file is big enough, forking a zsync is no big deal? but mac & win...
--------digression about zsync --------------------------------------------------------



server/protocol considerations.

HTTP:
	-- uses byte range feature of HTTP.
	-- FIXME: find samples from other email.


in SFTP/python/paramiko...
	-- there is readv( ... ) which allows to read subsets of a file.
	-- the read command in SFTP PROTOCOL spec has offset as a standard 
		argument of read
	

